---
title: "Efficient Dynamic Graph Learning with Refined Batch Parallel Training."
collection: publications
permalink: /publication/2025-04-30-RBT
excerpt: 'This paper is about a refined batch parallel training framework designed for efficient dynamic graph learning.'
date: 2025-04-30
venue: 'IJCAI'
# paperurl: 'https://XXX'
citation: 'Zhengzhao Feng, Rui Wang, Longjiao Zhang, Tongya Zheng, Ziqi Huang, Mingli Song, "Efficient Dynamic Graph Learning with Refined Batch Parallel Training." the 34th International Joint Conference on Artificial Intelligence (IJCAI 2025), Montreal, 16th â€“ 22nd August, 2025.'
---

Memory-based temporal graph neural networks (MTGNN) use node memory to store historical information, enabling efficient processing of large dynamic graphs through batch parallel training, with larger batch sizes leading to increased training efficiency. However, this approach overlooks the interdependency among edges within the same batch, leading to outdated memory states and reduced training accuracy. Previous studies have attempted to mitigate this issue through methods such as measuring memory loss, overlap training, and additional compensation modules. Despite these efforts, challenges persist, including imprecise coarse-grained memory loss measurement and ineffective compensation modules. To address these challenges, we propose the Refined Batch parallel Training (RBT) framework, which accurately evaluates intra-batch information loss and optimizes batch partitioning to minimize loss, enhancing the training process's effectiveness and efficiency. RBT also includes a precise and efficient memory compensation algorithm. Experimental results demonstrate RBT's superior performance compared to existing MTGNN frameworks like TGL, ETC, and PRES in terms of training efficiency and accuracy across various dynamic graph datasets. Our code is made publicly available at https://github.com/fengwudi/RBT.

